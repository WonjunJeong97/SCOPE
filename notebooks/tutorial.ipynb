{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e49f2d",
   "metadata": {},
   "source": [
    "# SCOPE Framework Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the SCOPE framework for bias-free evaluation of LLMs on multiple-choice questions.\n",
    "\n",
    "## Overview\n",
    "\n",
    "SCOPE consists of two main modules:\n",
    "1. Inverse-Positioning (IP): Measures and counteracts position bias\n",
    "2. Semantic-Spread (SS): Identifies and separates semantically similar distractors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9fdcd",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bfa0e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../')\n",
    "\n",
    "# Import SCOPE modules\n",
    "from src.data_preprocessing import load_fixed_datasets, print_sample_questions\n",
    "from src.scope.ip_module import (\n",
    "    generate_null_prompts, \n",
    "    measure_position_bias,\n",
    "    calculate_inverse_bias_distribution\n",
    ")\n",
    "from src.scope.ss_module import create_scope_processor\n",
    "from src.models import get_model\n",
    "from src.evaluate import evaluate_single_question, calculate_all_metrics\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad9cd2f",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "First, let's load the fixed datasets and examine some sample questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6243ece2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "csqa_data, mmlu_data = load_fixed_datasets(\n",
    "    '../data/fixed/csqa_500_fixed.json',\n",
    "    '../data/fixed/mmlu_500_fixed.json'\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(csqa_data)} CSQA questions\")\n",
    "print(f\"Loaded {len(mmlu_data)} MMLU questions\")\n",
    "\n",
    "# Show sample questions\n",
    "print(\"\\n=== CSQA Sample ===\")\n",
    "print_sample_questions(csqa_data[:2], n=2, dataset_name=\"CSQA\")\n",
    "\n",
    "print(\"\\n=== MMLU Sample ===\")\n",
    "print_sample_questions(mmlu_data[:2], n=2, dataset_name=\"MMLU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81b637b",
   "metadata": {},
   "source": [
    "## 3. Initialize Model\n",
    "\n",
    "Let's initialize a model for evaluation. Make sure you have the appropriate API key in your `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad462f74",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize model (change to your preferred model)\n",
    "model_name = \"gpt-3.5-turbo\"  # or \"claude-3-haiku\", \"gemini-1.5-flash\", etc.\n",
    "model = get_model(model_name)\n",
    "\n",
    "print(f\"Initialized model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d20747",
   "metadata": {},
   "source": [
    "## 4. Measure Position Bias (IP Module)\n",
    "\n",
    "The IP module measures the model's inherent position bias using null prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0443f72",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate null prompts for CSQA (5 choices)\n",
    "print(\"Generating null prompts...\")\n",
    "null_prompts = generate_null_prompts(num_choices=5, num_prompts=10)  # Use more for real evaluation\n",
    "\n",
    "# Show example null prompt\n",
    "print(\"\\nExample null prompt:\")\n",
    "print(f\"Question: {null_prompts[0]['question']}\")\n",
    "for label, text in null_prompts[0]['choices'].items():\n",
    "    print(f\"{label}) {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718e2885",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Measure position bias\n",
    "print(\"Measuring position bias (this may take a moment)...\")\n",
    "position_bias = measure_position_bias(model, null_prompts)\n",
    "\n",
    "print(\"\\nPosition bias distribution:\")\n",
    "for pos, prob in sorted(position_bias.items()):\n",
    "    print(f\"Position {pos}: {prob:.3f} ({prob*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6852fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate inverse bias distribution\n",
    "inverse_bias = calculate_inverse_bias_distribution(position_bias)\n",
    "\n",
    "print(\"\\nInverse bias distribution (for answer placement):\")\n",
    "for pos, prob in sorted(inverse_bias.items()):\n",
    "    print(f\"Position {pos}: {prob:.3f} ({prob*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a650cf",
   "metadata": {},
   "source": [
    "## 5. Apply SCOPE to Questions\n",
    "\n",
    "Now let's apply the complete SCOPE pipeline to rearrange question choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a3f45c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create SCOPE processor\n",
    "scope_processor = create_scope_processor(\n",
    "    inverse_bias,\n",
    "    sentence_bert_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Process a sample question\n",
    "sample_question = csqa_data[0]\n",
    "print(\"Original question:\")\n",
    "print(f\"Q: {sample_question['question']}\")\n",
    "print(\"Original choices:\")\n",
    "for label, text in sorted(sample_question['choices'].items()):\n",
    "    marker = \"[X]\" if label == sample_question['answer'] else \"[ ]\"\n",
    "    print(f\"{marker} {label}) {text}\")\n",
    "\n",
    "# Apply SCOPE\n",
    "processed_question = scope_processor(sample_question)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"After SCOPE processing:\")\n",
    "print(f\"Q: {processed_question['question']}\")\n",
    "print(\"Rearranged choices:\")\n",
    "for label, text in sorted(processed_question['choices'].items()):\n",
    "    original_label = processed_question['position_mapping'][label]\n",
    "    marker = \"[X]\" if original_label == sample_question['answer'] else \"[ ]\"\n",
    "    print(f\"{marker} {label}) {text} [originally {original_label}]\")\n",
    "\n",
    "print(f\"\\nCorrect answer placed at position: {processed_question['correct_position']}\")\n",
    "if 'ssd_info' in processed_question:\n",
    "    ssd_label = processed_question['ssd_info']['ssd_label']\n",
    "    ssd_position = processed_question['ssd_info'].get('ssd_position', 'N/A')\n",
    "    print(f\"SSD (most similar distractor): {ssd_label} placed at position {ssd_position}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e662d872",
   "metadata": {},
   "source": [
    "## 6. Evaluate with Multiple Trials\n",
    "\n",
    "SCOPE evaluates each question multiple times to measure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2eb7b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the processed question\n",
    "print(\"Evaluating question with 5 trials...\")\n",
    "evaluation_result = evaluate_single_question(\n",
    "    model,\n",
    "    processed_question,\n",
    "    num_trials=5,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(f\"\\nResponses: {evaluation_result['responses']}\")\n",
    "print(f\"Original labels: {evaluation_result['original_label_responses']}\")\n",
    "print(f\"Correct answer: {evaluation_result['correct_answer']}\")\n",
    "print(f\"Correct count: {evaluation_result['correct_count']}/5\")\n",
    "\n",
    "# Determine Pr and Co status\n",
    "if evaluation_result['correct_count'] >= 3:\n",
    "    print(\"Result: Pr-T (Prefers correct answer)\")\n",
    "else:\n",
    "    print(\"Result: Pr-F (Prefers incorrect answer)\")\n",
    "\n",
    "if evaluation_result['correct_count'] == 5:\n",
    "    print(\"Result: Co-T (Consistently correct)\")\n",
    "elif evaluation_result['most_common_count'] == 5 and \\\n",
    "     evaluation_result['most_common_response'] != evaluation_result['correct_answer']:\n",
    "    print(\"Result: Co-F (Consistently incorrect)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324c6ed6",
   "metadata": {},
   "source": [
    "## 7. Batch Evaluation\n",
    "\n",
    "Let's evaluate multiple questions and calculate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b972ce61",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Process and evaluate 5 questions (use more for real evaluation)\n",
    "from src.evaluate import run_evaluation_batch\n",
    "\n",
    "# Take first 5 questions\n",
    "test_questions = csqa_data[:5]\n",
    "\n",
    "# Apply SCOPE to all questions\n",
    "print(\"Processing questions with SCOPE...\")\n",
    "processed_questions = [scope_processor(q) for q in test_questions]\n",
    "\n",
    "# Evaluate all questions\n",
    "print(\"\\nEvaluating questions...\")\n",
    "evaluation_results = run_evaluation_batch(\n",
    "    model,\n",
    "    processed_questions,\n",
    "    num_trials=5,\n",
    "    temperature=1.0,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dacc7dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "metrics = calculate_all_metrics(\n",
    "    evaluation_results,\n",
    "    position_bias,\n",
    "    inverse_bias\n",
    ")\n",
    "\n",
    "print(\"\\n=== Evaluation Metrics ===\")\n",
    "print(f\"\\nPr-T (Prefer correct): {metrics['pr_co_metrics']['Pr-T']}\")\n",
    "print(f\"Pr-F (Prefer incorrect): {metrics['pr_co_metrics']['Pr-F']}\")\n",
    "print(f\"Co-T (Consistently correct): {metrics['pr_co_metrics']['Co-T']}\")\n",
    "print(f\"Co-F (Consistently incorrect): {metrics['pr_co_metrics']['Co-F']}\")\n",
    "\n",
    "print(f\"\\nAnswer Metrics:\")\n",
    "print(f\"  Precision: {metrics['answer_metrics']['Precision']:.3f}\")\n",
    "print(f\"  Recall: {metrics['answer_metrics']['Recall']:.3f}\")\n",
    "print(f\"  F1: {metrics['answer_metrics']['F1']:.3f}\")\n",
    "\n",
    "print(f\"\\nDistractor Metrics:\")\n",
    "print(f\"  Precision: {metrics['distractor_metrics']['Precision']:.3f}\")\n",
    "print(f\"  Recall: {metrics['distractor_metrics']['Recall']:.3f}\")\n",
    "print(f\"  F1: {metrics['distractor_metrics']['F1']:.3f}\")\n",
    "\n",
    "print(f\"\\nF1 Gap (Answer - Distractor): {metrics['f1_gap']:.3f}\")\n",
    "print(f\"Lucky-hit probability: {metrics['lucky_hit_probability']:.4f}\")\n",
    "print(f\"Pure skill: {metrics['pure_skill']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2c50ff",
   "metadata": {},
   "source": [
    "## 8. Ablation Study Example\n",
    "\n",
    "You can test different configurations to see the effect of each module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eff6a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test without SS module (IP only)\n",
    "print(\"Testing IP only (no semantic spread)...\")\n",
    "processed_ip_only = scope_processor(test_questions[0], use_ss=False)\n",
    "\n",
    "print(f\"Correct answer at: {processed_ip_only['correct_position']}\")\n",
    "print(\"Note: Without SS, semantically similar distractors are placed randomly\")\n",
    "\n",
    "# Test without IP module (uniform distribution)\n",
    "print(\"\\nTesting SS only (uniform position distribution)...\")\n",
    "uniform_dist = {chr(65+i): 0.2 for i in range(5)}  # Equal probability for all positions\n",
    "scope_ss_only = create_scope_processor(uniform_dist)\n",
    "processed_ss_only = scope_ss_only(test_questions[0])\n",
    "\n",
    "print(f\"Correct answer at: {processed_ss_only['correct_position']}\")\n",
    "print(\"Note: Without IP, answers are placed with equal probability at any position\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef31a5",
   "metadata": {},
   "source": [
    "## 9. Visualization\n",
    "\n",
    "Let's create a simple visualization of the position bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd440ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plot position bias\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Original position bias\n",
    "positions = sorted(position_bias.keys())\n",
    "bias_values = [position_bias[p] * 100 for p in positions]\n",
    "\n",
    "ax1.bar(positions, bias_values, color='skyblue', edgecolor='black')\n",
    "ax1.set_xlabel('Position')\n",
    "ax1.set_ylabel('Selection Rate (%)')\n",
    "ax1.set_title(f'{model_name} - Position Bias')\n",
    "ax1.set_ylim(0, max(bias_values) * 1.2)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(bias_values):\n",
    "    ax1.text(i, v + 1, f'{v:.1f}%', ha='center', fontsize=9)\n",
    "\n",
    "# Inverse bias distribution\n",
    "inverse_values = [inverse_bias[p] * 100 for p in positions]\n",
    "\n",
    "ax2.bar(positions, inverse_values, color='lightgreen', edgecolor='black')\n",
    "ax2.set_xlabel('Position')\n",
    "ax2.set_ylabel('Answer Placement Rate (%)')\n",
    "ax2.set_title('Inverse Bias (for Answer Placement)')\n",
    "ax2.set_ylim(0, max(inverse_values) * 1.2)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(inverse_values):\n",
    "    ax2.text(i, v + 1, f'{v:.1f}%', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ecba0",
   "metadata": {},
   "source": [
    "## 10. Running Full Evaluation\n",
    "\n",
    "For a complete evaluation, use the command line interface:\n",
    "\n",
    "```bash\n",
    "# Basic evaluation\n",
    "python ../src/main.py --model gpt-3.5-turbo --dataset csqa\n",
    "\n",
    "# Test mode (small sample)\n",
    "python ../src/main.py --model gpt-3.5-turbo --dataset both --test\n",
    "\n",
    "# Ablation study\n",
    "python ../src/main.py --model gpt-3.5-turbo --dataset both --ablation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275fd8d1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial demonstrated:\n",
    "1. How to measure position bias using null prompts\n",
    "2. How to calculate inverse bias distribution\n",
    "3. How to apply SCOPE to rearrange question choices\n",
    "4. How to evaluate questions multiple times\n",
    "5. How to calculate Pr/Co and Answer/Distractor F1 metrics\n",
    "\n",
    "The SCOPE framework helps achieve more reliable LLM evaluations by:\n",
    "- Removing position bias through inverse sampling\n",
    "- Separating semantically similar distractors\n",
    "- Measuring response consistency\n",
    "- Calculating pure skill by subtracting lucky-hit probability"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
